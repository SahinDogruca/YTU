import os
import cv2
import yaml
import albumentations as A
from pathlib import Path
from tqdm import tqdm
import shutil
from collections import defaultdict
import numpy as np


class ClassSpecificAugmentation:
    def __init__(self, data_yaml_path, output_folder_name, target_count=200):
        """
        Class-specific offline augmentation for polygon segmentation

        Args:
            data_yaml_path: data.yaml dosyasÄ±nÄ±n yolu
            output_folder_name: Yeni veri seti klasÃ¶r adÄ± (Ã¶rn: "data_v2")
            target_count: Her sÄ±nÄ±f iÃ§in hedef gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±
        """
        self.data_yaml_path = data_yaml_path
        self.output_folder_name = output_folder_name
        self.target_count = target_count
        self.load_config()
        self.setup_output_dirs()

    def load_config(self):
        """data.yaml dosyasÄ±nÄ± yÃ¼kle"""
        with open(self.data_yaml_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)

        # Base path'i al
        if "path" in self.config:
            base_path = Path(self.config["path"])
        else:
            base_path = Path(self.data_yaml_path).parent

        # Orijinal yollarÄ± oluÅŸtur
        train_rel = Path(self.config["train"])
        val_rel = Path(self.config["val"])
        test_rel = Path(self.config["test"])

        # EÄŸer 'images' klasÃ¶rÃ¼ zaten path'te varsa, parent'Ä±nÄ± al
        if train_rel.name == "images":
            self.train_path = base_path / train_rel.parent
        else:
            self.train_path = base_path / train_rel

        if val_rel.name == "images":
            self.val_path = base_path / val_rel.parent
        else:
            self.val_path = base_path / val_rel

        if test_rel.name == "images":
            self.test_path = base_path / test_rel.parent
        else:
            self.test_path = base_path / test_rel

        # Class isimlerini al
        self.class_names = self.config["names"]
        self.num_classes = self.config["nc"]

        print(f"âœ“ Config yÃ¼klendi: {self.num_classes} sÄ±nÄ±f")
        print(f"  Orijinal Train: {self.train_path}")
        print(f"  Orijinal Valid: {self.val_path}")
        print(f"  Orijinal Test: {self.test_path}")

    def setup_output_dirs(self):
        """Yeni output klasÃ¶rlerini oluÅŸtur"""
        # Output base path
        base_path = Path(self.data_yaml_path).parent
        self.output_base = base_path / self.output_folder_name

        # Yeni yollar
        self.output_train = self.output_base / "train"
        self.output_val = self.output_base / "valid"
        self.output_test = self.output_base / "test"

        print(f"\nâœ“ Yeni veri seti oluÅŸturulacak:")
        print(f"  Output Train: {self.output_train}")
        print(f"  Output Valid: {self.output_val}")
        print(f"  Output Test: {self.output_test}")

        # KlasÃ¶rleri oluÅŸtur
        for split_path in [self.output_train, self.output_val, self.output_test]:
            (split_path / "images").mkdir(parents=True, exist_ok=True)
            (split_path / "labels").mkdir(parents=True, exist_ok=True)

    def get_augmentation_pipeline(self, aggressive=True):
        """
        Augmentation pipeline oluÅŸtur (polygon-compatible)

        Args:
            aggressive: True ise daha agresif augmentation
        """
        if aggressive:
            # Train iÃ§in agresif augmentation
            return A.Compose(
                [
                    A.Rotate(limit=30, p=0.8),
                    A.HorizontalFlip(p=0.5),
                    A.VerticalFlip(p=0.5),
                    A.RandomBrightnessContrast(
                        brightness_limit=0.3, contrast_limit=0.3, p=0.8
                    ),
                    A.OneOf(
                        [
                            A.GaussianBlur(blur_limit=(3, 7), p=1.0),
                            A.MedianBlur(blur_limit=5, p=1.0),
                            A.MotionBlur(blur_limit=7, p=1.0),
                        ],
                        p=0.3,
                    ),
                    A.ElasticTransform(alpha=50, sigma=5, p=0.3),
                    A.CLAHE(clip_limit=4.0, p=0.5),
                    A.RandomGamma(gamma_limit=(80, 120), p=0.3),
                    A.GaussNoise(p=0.3),
                    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.3),
                ],
                keypoint_params=A.KeypointParams(format="xy", remove_invisible=False),
            )
        else:
            # Valid/Test iÃ§in hafif augmentation
            return A.Compose(
                [
                    A.Rotate(limit=10, p=0.5),
                    A.HorizontalFlip(p=0.5),
                    A.RandomBrightnessContrast(
                        brightness_limit=0.1, contrast_limit=0.1, p=0.5
                    ),
                ],
                keypoint_params=A.KeypointParams(format="xy", remove_invisible=False),
            )

    def parse_polygon_label(self, label_line):
        """
        Polygon formatÄ±ndaki label'Ä± parse et

        Args:
            label_line: "class_id x1 y1 x2 y2 x3 y3 ..." formatÄ±nda string

        Returns:
            class_id, keypoints (list of tuples)
        """
        parts = label_line.strip().split()
        if len(parts) < 7:  # En az class + 3 nokta (6 koordinat)
            return None, None

        class_id = int(parts[0])
        coords = [float(x) for x in parts[1:]]

        # KoordinatlarÄ± (x, y) tuple'larÄ±na Ã§evir
        keypoints = [(coords[i], coords[i + 1]) for i in range(0, len(coords), 2)]

        return class_id, keypoints

    def keypoints_to_polygon_string(self, class_id, keypoints):
        """
        Keypoint'leri YOLO polygon formatÄ±na Ã§evir

        Args:
            class_id: SÄ±nÄ±f ID'si
            keypoints: [(x1, y1), (x2, y2), ...] formatÄ±nda liste

        Returns:
            "class_id x1 y1 x2 y2 ..." formatÄ±nda string
        """
        coords = []
        for x, y in keypoints:
            # KoordinatlarÄ± [0, 1] aralÄ±ÄŸÄ±nda tut
            x = max(0.0, min(1.0, x))
            y = max(0.0, min(1.0, y))
            coords.extend([f"{x:.6f}", f"{y:.6f}"])

        return f"{class_id} {' '.join(coords)}"

    def analyze_class_distribution(self, dataset_path):
        """Veri setindeki sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± analiz et"""
        images_path = dataset_path / "images"
        labels_path = dataset_path / "labels"

        class_counts = defaultdict(int)
        class_images = defaultdict(list)

        # TÃ¼m label dosyalarÄ±nÄ± tara
        for label_file in labels_path.glob("*.txt"):
            with open(label_file, "r") as f:
                lines = f.readlines()

            # Bu gÃ¶rÃ¼ntÃ¼deki sÄ±nÄ±flarÄ± say
            image_classes = set()
            for line in lines:
                if line.strip():
                    class_id, _ = self.parse_polygon_label(line)
                    if class_id is not None:
                        image_classes.add(class_id)

            # Her sÄ±nÄ±f iÃ§in bu gÃ¶rÃ¼ntÃ¼yÃ¼ kaydet
            for class_id in image_classes:
                class_counts[class_id] += 1
                class_images[class_id].append(label_file.stem)

        return class_counts, class_images

    def augment_image(
        self,
        img_path,
        label_path,
        transform,
        output_images_dir,
        output_labels_dir,
        output_name,
    ):
        """
        Tek bir gÃ¶rÃ¼ntÃ¼yÃ¼ ve polygon annotation'Ä±nÄ± augment et

        Args:
            img_path: Orijinal gÃ¶rÃ¼ntÃ¼ yolu
            label_path: Orijinal label yolu
            transform: Augmentation pipeline
            output_images_dir: Output images klasÃ¶rÃ¼
            output_labels_dir: Output labels klasÃ¶rÃ¼
            output_name: Yeni dosya adÄ± (uzantÄ±sÄ±z)
        """
        # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle
        img = cv2.imread(str(img_path))
        if img is None:
            print(f"  âš  GÃ¶rÃ¼ntÃ¼ yÃ¼klenemedi: {img_path}")
            return False

        height, width = img.shape[:2]

        # Label'larÄ± yÃ¼kle (polygon format)
        polygons = []  # Her eleman: (class_id, keypoints_list)

        if label_path.exists():
            with open(label_path, "r") as f:
                for line in f:
                    if line.strip():
                        class_id, keypoints = self.parse_polygon_label(line)
                        if class_id is not None and keypoints is not None:
                            # Normalized koordinatlarÄ± pixel koordinatlarÄ±na Ã§evir
                            pixel_keypoints = [
                                (x * width, y * height) for x, y in keypoints
                            ]
                            polygons.append((class_id, pixel_keypoints))

        if not polygons:
            print(f"  âš  Annotation bulunamadÄ±: {label_path}")
            return False

        try:
            # TÃ¼m keypoint'leri tek bir liste halinde birleÅŸtir
            all_keypoints = []
            keypoint_class_ids = []
            keypoint_counts = []

            for class_id, keypoints in polygons:
                all_keypoints.extend(keypoints)
                keypoint_class_ids.extend([class_id] * len(keypoints))
                keypoint_counts.append(len(keypoints))

            # Augmentation uygula
            augmented = transform(image=img, keypoints=all_keypoints)

            # Augmented gÃ¶rÃ¼ntÃ¼yÃ¼ kaydet
            new_img_path = output_images_dir / f"{output_name}{img_path.suffix}"
            cv2.imwrite(str(new_img_path), augmented["image"])

            # Augmented keypoint'leri geri polygon'lara ayÄ±r
            aug_keypoints = augmented["keypoints"]
            new_height, new_width = augmented["image"].shape[:2]

            # Label'Ä± kaydet
            new_label_path = output_labels_dir / f"{output_name}.txt"
            with open(new_label_path, "w") as f:
                start_idx = 0
                for i, count in enumerate(keypoint_counts):
                    end_idx = start_idx + count
                    polygon_keypoints = aug_keypoints[start_idx:end_idx]
                    class_id = keypoint_class_ids[start_idx]

                    # Pixel koordinatlarÄ±nÄ± normalize et
                    normalized_keypoints = [
                        (x / new_width, y / new_height) for x, y in polygon_keypoints
                    ]

                    # YOLO polygon formatÄ±nda yaz
                    polygon_str = self.keypoints_to_polygon_string(
                        class_id, normalized_keypoints
                    )
                    f.write(polygon_str + "\n")

                    start_idx = end_idx

            return True

        except Exception as e:
            print(f"  âš  Augmentation hatasÄ± ({img_path.name}): {e}")
            return False

    def copy_original_files(self, src_path, dst_path, split_name):
        """Orijinal dosyalarÄ± yeni klasÃ¶re kopyala"""
        print(f"\nğŸ“‹ Orijinal {split_name} dosyalarÄ± kopyalanÄ±yor...")

        src_images = src_path / "images"
        src_labels = src_path / "labels"
        dst_images = dst_path / "images"
        dst_labels = dst_path / "labels"

        # GÃ¶rÃ¼ntÃ¼leri kopyala
        for img_file in src_images.glob("*"):
            if img_file.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp"]:
                shutil.copy2(img_file, dst_images / img_file.name)

        # Label'larÄ± kopyala
        for label_file in src_labels.glob("*.txt"):
            shutil.copy2(label_file, dst_labels / label_file.name)

        print(f"  âœ“ {split_name} dosyalarÄ± kopyalandÄ±")

    def augment_dataset(self, src_path, dst_path, split_name, augment_all=False):
        """
        Bir veri setini augment et

        Args:
            src_path: Kaynak veri seti yolu
            dst_path: Hedef veri seti yolu
            split_name: 'train', 'valid' veya 'test'
            augment_all: True ise tÃ¼m sÄ±nÄ±flarÄ± augment et
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {split_name.upper()} seti iÅŸleniyor...")
        print(f"{'='*60}")

        src_images = src_path / "images"
        src_labels = src_path / "labels"
        dst_images = dst_path / "images"
        dst_labels = dst_path / "labels"

        if not src_images.exists() or not src_labels.exists():
            print(f"âš  Kaynak klasÃ¶rler bulunamadÄ±: {src_path}")
            return

        # Ã–nce orijinal dosyalarÄ± kopyala
        self.copy_original_files(src_path, dst_path, split_name)

        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± analiz et
        class_counts, class_images = self.analyze_class_distribution(src_path)

        print(f"\nğŸ“ˆ Orijinal daÄŸÄ±lÄ±m:")
        total_images = (
            len(list(src_images.glob("*.jpg")))
            + len(list(src_images.glob("*.png")))
            + len(list(src_images.glob("*.jpeg")))
            + len(list(src_images.glob("*.bmp")))
        )
        for class_id in sorted(class_counts.keys()):
            count = class_counts[class_id]
            percentage = (count / total_images * 100) if total_images > 0 else 0
            class_name = (
                self.class_names[class_id]
                if class_id < len(self.class_names)
                else f"Class_{class_id}"
            )
            print(
                f"  {class_name} (Class {class_id}): {count} gÃ¶rÃ¼ntÃ¼ ({percentage:.1f}%)"
            )

        # Augmentation pipeline seÃ§
        if split_name == "train":
            transform = self.get_augmentation_pipeline(aggressive=True)
        else:
            transform = self.get_augmentation_pipeline(aggressive=False)

        # Augmentation yap
        print(f"\nğŸ”„ Augmentation baÅŸlÄ±yor...")
        augmented_counts = defaultdict(int)

        if augment_all:
            # Valid/Test iÃ§in: TÃ¼m sÄ±nÄ±flarÄ± hafifÃ§e augment et
            multiplier = 2
            print(f"  TÃ¼m sÄ±nÄ±flar {multiplier}x Ã§oÄŸaltÄ±lacak")

            for img_file in tqdm(
                list(src_images.glob("*.jpg"))
                + list(src_images.glob("*.png"))
                + list(src_images.glob("*.jpeg"))
                + list(src_images.glob("*.bmp")),
                desc=f"  {split_name}",
            ):
                label_file = src_labels / f"{img_file.stem}.txt"

                for i in range(multiplier - 1):
                    output_name = f"{img_file.stem}_aug{i+1}"
                    success = self.augment_image(
                        img_file,
                        label_file,
                        transform,
                        dst_images,
                        dst_labels,
                        output_name,
                    )
                    if success:
                        augmented_counts["all"] += 1
        else:
            # Train iÃ§in: Sadece az olan sÄ±nÄ±flarÄ± Ã§oÄŸalt
            for class_id in sorted(class_counts.keys()):
                count = class_counts[class_id]
                class_name = (
                    self.class_names[class_id]
                    if class_id < len(self.class_names)
                    else f"Class_{class_id}"
                )

                if count >= self.target_count:
                    print(f"  âœ“ {class_name}: {count} gÃ¶rÃ¼ntÃ¼ (yeterli)")
                    continue

                # KaÃ§ kat Ã§oÄŸaltmak gerekiyor?
                multiplier = max(2, (self.target_count // count) + 1)
                needed = self.target_count - count

                print(
                    f"  ğŸ¯ {class_name}: {count} â†’ {min(count * multiplier, self.target_count)} gÃ¶rÃ¼ntÃ¼ ({multiplier}x)"
                )

                # Bu sÄ±nÄ±fa ait gÃ¶rÃ¼ntÃ¼leri augment et
                class_image_list = class_images[class_id]
                augmented = 0

                for img_name in tqdm(
                    class_image_list, desc=f"    {class_name}", leave=False
                ):
                    # GÃ¶rÃ¼ntÃ¼ dosyasÄ±nÄ± bul
                    img_file = None
                    for ext in [".jpg", ".png", ".jpeg", ".bmp"]:
                        potential_path = src_images / f"{img_name}{ext}"
                        if potential_path.exists():
                            img_file = potential_path
                            break

                    if img_file is None:
                        continue

                    label_file = src_labels / f"{img_name}.txt"

                    # multiplier kadar augmented versiyon oluÅŸtur
                    for i in range(multiplier - 1):
                        if augmented >= needed:
                            break

                        output_name = f"{img_name}_aug{i+1}_c{class_id}"
                        success = self.augment_image(
                            img_file,
                            label_file,
                            transform,
                            dst_images,
                            dst_labels,
                            output_name,
                        )

                        if success:
                            augmented += 1
                            augmented_counts[class_id] += 1

                    if augmented >= needed:
                        break

        # Yeni daÄŸÄ±lÄ±mÄ± gÃ¶ster
        print(f"\nâœ… Augmentation tamamlandÄ±!")
        new_class_counts, _ = self.analyze_class_distribution(dst_path)

        print(f"\nğŸ“Š Yeni daÄŸÄ±lÄ±m:")
        new_total = (
            len(list(dst_images.glob("*.jpg")))
            + len(list(dst_images.glob("*.png")))
            + len(list(dst_images.glob("*.jpeg")))
            + len(list(dst_images.glob("*.bmp")))
        )
        for class_id in sorted(new_class_counts.keys()):
            count = new_class_counts[class_id]
            old_count = class_counts.get(class_id, 0)
            percentage = (count / new_total * 100) if new_total > 0 else 0
            change = count - old_count
            class_name = (
                self.class_names[class_id]
                if class_id < len(self.class_names)
                else f"Class_{class_id}"
            )
            print(
                f"  {class_name} (Class {class_id}): {count} gÃ¶rÃ¼ntÃ¼ ({percentage:.1f}%) [+{change}]"
            )

        print(f"\nğŸ“ˆ Toplam: {total_images} â†’ {new_total} gÃ¶rÃ¼ntÃ¼")

    def create_new_yaml(self):
        """Yeni data.yaml dosyasÄ± oluÅŸtur"""
        new_yaml_path = self.output_base / "data.yaml"

        new_config = self.config.copy()
        new_config["path"] = str(self.output_base.absolute())
        new_config["train"] = "train/images"
        new_config["val"] = "valid/images"
        new_config["test"] = "test/images"

        with open(new_yaml_path, "w", encoding="utf-8") as f:
            yaml.dump(new_config, f, default_flow_style=False, allow_unicode=True)

        print(f"\nâœ“ Yeni data.yaml oluÅŸturuldu: {new_yaml_path}")

    def run(self, augment_train=True, augment_valid=False, augment_test=False):
        """
        TÃ¼m augmentation iÅŸlemini Ã§alÄ±ÅŸtÄ±r

        Args:
            augment_train: Train setini augment et (class-specific)
            augment_valid: Valid setini augment et (tÃ¼m sÄ±nÄ±flar, hafif)
            augment_test: Test setini augment et (tÃ¼m sÄ±nÄ±flar, hafif)
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ POLYGON SEGMENTATION AUGMENTATION")
        print(f"{'='*60}")
        print(f"Output folder: {self.output_folder_name}")
        print(f"Target count: {self.target_count} gÃ¶rÃ¼ntÃ¼/sÄ±nÄ±f (train iÃ§in)")
        print(f"Train augmentation: {'âœ“' if augment_train else 'âœ—'}")
        print(f"Valid augmentation: {'âœ“' if augment_valid else 'âœ—'}")
        print(f"Test augmentation: {'âœ“' if augment_test else 'âœ—'}")

        # Onay al
        print(f"\nâš  Yeni veri seti oluÅŸturulacak: {self.output_base}")
        confirm = input("Devam etmek istiyor musunuz? (y/n): ")
        if confirm.lower() != "y":
            print("Ä°ÅŸlem iptal edildi.")
            return

        # Train setini iÅŸle
        if augment_train:
            self.augment_dataset(
                self.train_path, self.output_train, "train", augment_all=False
            )

        # Valid setini iÅŸle
        if augment_valid:
            self.augment_dataset(
                self.val_path, self.output_val, "valid", augment_all=True
            )
        else:
            # Sadece kopyala
            self.copy_original_files(self.val_path, self.output_val, "valid")

        # Test setini iÅŸle
        if augment_test:
            print("\nâš  UYARI: Test setini augment etmek Ã¶nerilmez!")
            confirm = input("Devam etmek istediÄŸinizden emin misiniz? (yes/no): ")
            if confirm.lower() == "yes":
                self.augment_dataset(
                    self.test_path, self.output_test, "test", augment_all=True
                )
            else:
                self.copy_original_files(self.test_path, self.output_test, "test")
        else:
            # Sadece kopyala
            self.copy_original_files(self.test_path, self.output_test, "test")

        # Yeni data.yaml oluÅŸtur
        self.create_new_yaml()

        print(f"\n{'='*60}")
        print(f"âœ… TÃœM Ä°ÅLEMLER TAMAMLANDI!")
        print(f"{'='*60}")
        print(f"Yeni veri seti: {self.output_base}")
        print(f"Yeni data.yaml: {self.output_base / 'data.yaml'}")


if __name__ == "__main__":
    # KullanÄ±m
    augmenter = ClassSpecificAugmentation(
        data_yaml_path="data.yaml",  # Orijinal data.yaml yolu
        output_folder_name="data/data_v2_aug",  # Yeni klasÃ¶r adÄ±
        target_count=200,  # Her sÄ±nÄ±ftan hedef gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±
    )

    # Ã‡alÄ±ÅŸtÄ±r
    augmenter.run(
        augment_train=True,  # Train setini augment et (class-specific)
        augment_valid=False,  # Valid setini augment etme (Ã¶nerilir)
        augment_test=False,  # Test setini augment etme (kesinlikle Ã¶nerilmez!)
    )
